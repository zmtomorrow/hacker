{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching up to  1000 records...\n",
      "Got 503. Retrying after 30 seconds.\n",
      "fetching up to  1000 records...\n",
      "fetching up to  2000 records...\n",
      "Got 503. Retrying after 30 seconds.\n",
      "fetching up to  2000 records...\n",
      "fetching is completed in 76.4 seconds.\n",
      "Total number of records 597\n"
     ]
    }
   ],
   "source": [
    "import arxivscraper\n",
    "scraper = arxivscraper.Scraper(category='cs', date_from='2023-03-07',date_until='2023-03-08', filters={'categories':['cs.lg']})\n",
    "output = scraper.scrape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                                              title  \\\n",
      "0  2303.04115  predicted embedding power regression for large...   \n",
      "1  2303.04117  validation of a hospital digital twin with mac...   \n",
      "2  2303.04118  a multiplicative value function for safe and e...   \n",
      "3  2303.04124  wigner kernels: body-ordered equivariant machi...   \n",
      "4  2303.04129  foundation models for decision making: problem...   \n",
      "5  2303.04132  exploiting asymmetry for synthetic training da...   \n",
      "6  2303.04136  domain randomization for robust, affordable an...   \n",
      "7  2303.04142  from copilot to pilot: towards ai supported so...   \n",
      "8  2303.04143  can we scale transformers to predict parameter...   \n",
      "9  2303.04145     benign overfitting for two-layer relu networks   \n",
      "\n",
      "                                            abstract  \n",
      "0  out-of-distribution (ood) inputs can compromis...  \n",
      "1  recently there has been a surge of interest in...  \n",
      "2  an emerging field of sequential decision probl...  \n",
      "3  machine-learning models based on a point-cloud...  \n",
      "4  foundation models pretrained on diverse data a...  \n",
      "5  large language models (llms) show great potent...  \n",
      "6  soft robots are becoming extremely popular tha...  \n",
      "7  ai-supported programming has arrived, as shown...  \n",
      "8  pretraining a neural network on a large datase...  \n",
      "9  modern deep learning models with great express...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cols = ['id', 'title',  'abstract']\n",
    "df = pd.DataFrame(list(output)[-10:],columns=cols)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'elodin: naming concepts in embedding spaces', 'id': '2303.04001', 'abstract': \"despite recent advancements, the field of text-to-image synthesis still suffers from lack of fine-grained control. using only text, it remains challenging to deal with issues such as concept coherence and concept contamination. we propose a method to enhance control by generating specific concepts that can be reused throughout multiple images, effectively expanding natural language with new words that can be combined much like a painter's palette. unlike previous contributions, our method does not copy visuals from input data and can generate concepts through text alone. we perform a set of comparisons that finds our method to be a significant improvement over text-only prompts.\", 'categories': 'cs.cv cs.cl cs.gr cs.lg', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['rodrigo mello', 'filipe calegario', 'geber ramalho'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04001'}, {'title': 'exploration via epistemic value estimation', 'id': '2303.04012', 'abstract': 'how to efficiently explore in reinforcement learning is an open problem. many exploration algorithms employ the epistemic uncertainty of their own value predictions -- for instance to compute an exploration bonus or upper confidence bound. unfortunately the required uncertainty is difficult to estimate in general with function approximation.   we propose epistemic value estimation (eve): a recipe that is compatible with sequential decision making and with neural network function approximators. it equips agents with a tractable posterior over all their parameters from which epistemic value uncertainty can be computed efficiently.   we use the recipe to derive an epistemic q-learning agent and observe competitive performance on a series of benchmarks. experiments confirm that the eve recipe facilitates efficient exploration in hard exploration tasks.', 'categories': 'cs.lg cs.ai stat.ml', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['simon schmitt', 'john shawe-taylor', 'hado van hasselt'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04012'}, {'title': 'decoupling skill learning from robotic control for generalizable object   manipulation', 'id': '2303.04016', 'abstract': \"recent works in robotic manipulation through reinforcement learning (rl) or imitation learning (il) have shown potential for tackling a range of tasks e.g., opening a drawer or a cupboard. however, these techniques generalize poorly to unseen objects. we conjecture that this is due to the high-dimensional action space for joint control. in this paper, we take an alternative approach and separate the task of learning 'what to do' from 'how to do it' i.e., whole-body control. we pose the rl problem as one of determining the skill dynamics for a disembodied virtual manipulator interacting with articulated objects. the whole-body robotic kinematic control is optimized to execute the high-dimensional joint motion to reach the goals in the workspace. it does so by solving a quadratic programming (qp) model with robotic singularity and kinematic constraints. our experiments on manipulating complex articulated objects show that the proposed approach is more generalizable to unseen objects with large intra-class variations, outperforming previous approaches. the evaluation results indicate that our approach generates more compliant robotic motion and outperforms the pure rl and il baselines in task success rates.\", 'categories': 'cs.ro cs.cv cs.lg', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['kai lu', 'bo yang', 'bing wang', 'andrew markham'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04016'}, {'title': 'when is importance weighting correction needed for covariate shift   adaptation?', 'id': '2303.04020', 'abstract': 'this paper investigates when the importance weighting (iw) correction is needed to address covariate shift, a common situation in supervised learning where the input distributions of training and test data differ. classic results show that the iw correction is needed when the model is parametric and misspecified. in contrast, recent results indicate that the iw correction may not be necessary when the model is nonparametric and well-specified. we examine the missing case in the literature where the model is nonparametric and misspecified, and show that the iw correction is needed for obtaining the best approximation of the true unknown function for the test distribution. we do this by analyzing iw-corrected kernel ridge regression, covering a variety of settings, including parametric and nonparametric models, well-specified and misspecified settings, and arbitrary weighting functions.', 'categories': 'stat.ml cs.lg', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['davit gogolashvili', 'matteo zecchin', 'motonobu kanagawa', 'marios kountouris', 'maurizio filippone'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04020'}, {'title': 'pyxab -- a python library for $\\\\mathcal{x}$-armed bandit and online   blackbox optimization algorithms', 'id': '2303.04030', 'abstract': 'we introduce a python open-source library for $\\\\mathcal{x}$-armed bandit and online blackbox optimization named pyxab. pyxab contains the implementations for more than 10 $\\\\mathcal{x}$-armed bandit algorithms, such as hoo, stosoo, hct, and the most recent works gpo and vhct. pyxab also provides the most commonly-used synthetic objectives to evaluate the performance of different algorithms and the various choices of the hierarchical partitions on the parameter space. the online documentation for pyxab includes clear instructions for installation, straight-forward examples, detailed feature descriptions, and a complete reference of the api. pyxab is released under the mit license in order to encourage both academic and industrial usage. the library can be directly installed from pypi with its source code available at https://github.com/williamlwj/pyxab', 'categories': 'stat.ml cs.ai cs.lg cs.se', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['wenjie li', 'haoze li', 'jean honorio', 'qifan song'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04030'}, {'title': 'root cause identification for collective anomalies in time series given   an acyclic summary causal graph with loops', 'id': '2303.04038', 'abstract': 'this paper presents an approach for identifying the root causes of collective anomalies given observational time series and an acyclic summary causal graph which depicts an abstraction of causal relations present in a dynamic system at its normal regime. the paper first shows how the problem of root cause identification can be divided into many independent subproblems by grouping related anomalies using d-separation. further, it shows how, under this setting, some root causes can be found directly from the graph and from the time of appearance of anomalies. finally, it shows, how the rest of the root causes can be found by comparing direct causal effects in the normal and in the anomalous regime. to this end, temporal adaptations of the back-door and the single-door criterions are introduced. extensive experiments conducted on both simulated and real-world datasets demonstrate the effectiveness of the proposed method.', 'categories': 'cs.ai cs.lg', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['charles k. assaad', 'imad ez-zejjari', 'lei zan'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04038'}, {'title': 'uncertainty quantification of spatiotemporal travel demand with   probabilistic graph neural networks', 'id': '2303.04040', 'abstract': 'recent studies have significantly improved the prediction accuracy of travel demand using graph neural networks. however, these studies largely ignored uncertainty that inevitably exists in travel demand prediction. to fill this gap, this study proposes a framework of probabilistic graph neural networks (prob-gnn) to quantify the spatiotemporal uncertainty of travel demand. this prob-gnn framework is substantiated by deterministic and probabilistic assumptions, and empirically applied to the task of predicting the transit and ridesharing demand in chicago. we found that the probabilistic assumptions (e.g. distribution tail, support) have a greater impact on uncertainty prediction than the deterministic ones (e.g. deep modules, depth). among the family of prob-gnns, the gnns with truncated gaussian and laplace distributions achieve the highest performance in transit and ridesharing data. even under significant domain shifts, prob-gnns can predict the ridership uncertainty in a stable manner, when the models are trained on pre-covid data and tested across multiple periods during and after the covid-19 pandemic. prob-gnns also reveal the spatiotemporal pattern of uncertainty, which is concentrated on the afternoon peak hours and the areas with large travel volumes. overall, our findings highlight the importance of incorporating randomness into deep learning for spatiotemporal ridership prediction. future research should continue to investigate versatile probabilistic assumptions to capture behavioral randomness, and further develop methods to quantify uncertainty to build resilient cities.', 'categories': 'cs.lg stat.ap stat.ml', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['qingyi wang', 'shenhao wang', 'dingyi zhuang', 'haris koutsopoulos', 'jinhua zhao'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04040'}, {'title': 'visual abstraction and reasoning through language', 'id': '2303.04091', 'abstract': 'while artificial intelligence (ai) models have achieved human or even superhuman performance in narrowly defined applications, they still struggle to show signs of broader and more flexible intelligence. the abstraction and reasoning corpus (arc), introduced by fran\\\\c{c}ois chollet, aims to assess how close ai systems are to human-like cognitive abilities. most current approaches rely on carefully handcrafted domain-specific languages (dsls), which are used to brute-force solutions to the tasks present in arc. in this work, we propose a general framework for solving arc based on natural language descriptions of the tasks. while not yet beating state-of-the-art dsl models on arc, we demonstrate the immense potential of our approach hinted at by the ability to solve previously unsolved tasks.', 'categories': 'cs.ai cs.cl cs.lg', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['giacomo camposampiero', 'loic houmard', 'benjamin estermann', 'joël mathys', 'roger wattenhofer'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04091'}, {'title': 'mastering strategy card game (legends of code and magic) via end-to-end   policy and optimistic smooth fictitious play', 'id': '2303.04096', 'abstract': 'deep reinforcement learning combined with fictitious play shows impressive results on many benchmark games, most of which are, however, single-stage. in contrast, real-world decision making problems may consist of multiple stages, where the observation spaces and the action spaces can be completely different across stages. we study a two-stage strategy card game legends of code and magic and propose an end-to-end policy to address the difficulties that arise in multi-stage game. we also propose an optimistic smooth fictitious play algorithm to find the nash equilibrium for the two-player game. our approach wins double championships of cog2022 competition. extensive studies verify and show the advancement of our approach.', 'categories': 'cs.lg cs.ai cs.gt', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['wei xi', 'yongxin zhang', 'changnan xiao', 'xuefeng huang', 'shihong deng', 'haowei liang', 'jie chen', 'peng sun'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04096'}, {'title': 'an inception-residual-based architecture with multi-objective loss for   detecting respiratory anomalies', 'id': '2303.04104', 'abstract': 'this paper presents a deep learning system applied for detecting anomalies from respiratory sound recordings. initially, our system begins with audio feature extraction using gammatone and continuous wavelet transformation. this step aims to transform the respiratory sound input into a two-dimensional spectrogram where both spectral and temporal features are presented. then, our proposed system integrates inception-residual-based backbone models combined with multi-head attention and multi-objective loss to classify respiratory anomalies. in this work, we conducted experiments over the benchmark dataset of sprsound (the open-source sjtu paediatric respiratory sound) proposed by the ieee biocas 2022 challenge. as regards the score computed by an average between the average score and harmonic score, our proposed system gained significant improvements of 9.7%, 15.8%, 17.0%, and 9.4% in task 1-1, task 1-2, task 2-1, and task 2-2 compared to the challenge baseline system. notably, we achieved the top-1 performance in task 2-1 with the highest score of 73.7%.', 'categories': 'cs.sd cs.lg eess.as q-bio.qm', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['dat ngo', 'lam pham', 'huy phan', 'minh tran', 'delaram jarchi', 'sefki kolozali'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04104'}, {'title': 'introspective cross-attention probing for lightweight transfer of   pre-trained models', 'id': '2303.04105', 'abstract': 'we propose inca, a lightweight method for transfer learning that cross-attends to any activation layer of a pre-trained model. during training, inca uses a single forward pass to extract multiple activations, which are passed to external cross-attention adapters, trained anew and combined or selected for downstream tasks. we show that, even when selecting a single top-scoring adapter, inca achieves performance comparable to full fine-tuning, at a cost comparable to fine-tuning just the last layer. for example, with a cross-attention probe 1.3% the size of a pre-trained vit-l/16 model, we achieve performance within 0.2% of the full fine-tuning paragon at 51% training cost of the baseline, on average across 11 downstream classification tasks. unlike other forms of efficient adaptation, inca does not require backpropagating through the pre-trained model, thus leaving its execution unaltered at both training and inference. the versatility of inca is best illustrated in fine-grained tasks, which may require accessing information absent in the last layer but accessible in intermediate layer activations. since the backbone is fixed, inca allows parallel ensembling as well as parallel execution of multiple tasks. inca achieves state-of-the-art performance in the imagenet-to-sketch multi-task benchmark.', 'categories': 'cs.lg cs.cv', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['yonatan dukler', 'alessandro achille', 'hao yang', 'varsha vivek', 'luca zancato', 'ben bowman', 'avinash ravichandran', 'charless fowlkes', 'ashwin swaminathan', 'stefano soatto'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04105'}, {'title': 'predicted embedding power regression for large-scale out-of-distribution   detection', 'id': '2303.04115', 'abstract': 'out-of-distribution (ood) inputs can compromise the performance and safety of real world machine learning systems. while many methods exist for ood detection and work well on small scale datasets with lower resolution and few classes, few methods have been developed for large-scale ood detection. existing large-scale methods generally depend on maximum classification probability, such as the state-of-the-art grouped softmax method. in this work, we develop a novel approach that calculates the probability of the predicted class label based on label distributions learned during the training process. our method performs better than current state-of-the-art methods with only a negligible increase in compute cost. we evaluate our method against contemporary methods across $14$ datasets and achieve a statistically significant improvement with respect to auroc (84.2 vs 82.4) and aupr (96.2 vs 93.7).', 'categories': 'cs.cv cs.lg', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['hong yang', 'william gebhardt', 'alexander g. ororbia', 'travis desell'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04115'}, {'title': 'validation of a hospital digital twin with machine learning', 'id': '2303.04117', 'abstract': 'recently there has been a surge of interest in developing digital twins of process flows in healthcare to better understand bottlenecks and areas of improvement. a key challenge is in the validation process. we describe a work in progress for a digital twin using an agent based simulation model for determining bed turnaround time for patients in hospitals. we employ a strategy using machine learning for validating the model and implementing sensitivity analysis.', 'categories': 'cs.ai cs.lg', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['muhammad aurangzeb ahmad', 'vijay chickarmane', 'farinaz ali sabzpour', 'nima shariari', 'taposh roy'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04117'}, {'title': 'a multiplicative value function for safe and efficient reinforcement   learning', 'id': '2303.04118', 'abstract': 'an emerging field of sequential decision problems is safe reinforcement learning (rl), where the objective is to maximize the reward while obeying safety constraints. being able to handle constraints is essential for deploying rl agents in real-world environments, where constraint violations can harm the agent and the environment. to this end, we propose a safe model-free rl algorithm with a novel multiplicative value function consisting of a safety critic and a reward critic. the safety critic predicts the probability of constraint violation and discounts the reward critic that only estimates constraint-free returns. by splitting responsibilities, we facilitate the learning task leading to increased sample efficiency. we integrate our approach into two popular rl algorithms, proximal policy optimization and soft actor-critic, and evaluate our method in four safety-focused environments, including classical rl benchmarks augmented with safety constraints and robot navigation tasks with images and raw lidar scans as observations. finally, we make the zero-shot sim-to-real transfer where a differential drive robot has to navigate through a cluttered room. our code can be found at https://github.com/nikeke19/safe-mult-rl.', 'categories': 'cs.ro cs.lg', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['nick bührer', 'zhejun zhang', 'alexander liniger', 'fisher yu', 'luc van gool'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04118'}, {'title': 'wigner kernels: body-ordered equivariant machine learning without a   basis', 'id': '2303.04124', 'abstract': \"machine-learning models based on a point-cloud representation of a physical object are ubiquitous in scientific applications and particularly well-suited to the atomic-scale description of molecules and materials. among the many different approaches that have been pursued, the description of local atomic environments in terms of their neighbor densities has been used widely and very succesfully. we propose a novel density-based method which involves computing ``wigner kernels''. these are fully equivariant and body-ordered kernels that can be computed iteratively with a cost that is independent of the radial-chemical basis and grows only linearly with the maximum body-order considered. this is in marked contrast to feature-space models, which comprise an exponentially-growing number of terms with increasing order of correlations. we present several examples of the accuracy of models based on wigner kernels in chemical applications, for both scalar and tensorial targets, reaching state-of-the-art accuracy on the popular qm9 benchmark dataset, and we discuss the broader relevance of these ideas to equivariant geometric machine-learning.\", 'categories': 'physics.chem-ph cs.lg stat.ml', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['filippo bigi', 'sergey n. pozdnyakov', 'michele ceriotti'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04124'}, {'title': 'foundation models for decision making: problems, methods, and   opportunities', 'id': '2303.04129', 'abstract': 'foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. when such models are deployed in real world environments, they inevitably interface with other entities and agents. for example, language models are often used to interact with human beings through dialogue, and visual perception models are used to autonomously navigate neighborhood streets. in response to these developments, new paradigms are emerging for training foundation models to interact with other agents and perform long-term reasoning. these paradigms leverage the existence of ever-larger datasets curated for multimodal, multitask, and generalist interaction. research at the intersection of foundation models and decision making holds tremendous promise for creating powerful new systems that can interact effectively across a diverse range of applications such as dialogue, autonomous driving, healthcare, education, and robotics. in this manuscript, we examine the scope of foundation models for decision making, and provide conceptual tools and technical background for understanding the problem space and exploring new research directions. we review recent approaches that ground foundation models in practical decision making applications through a variety of methods such as prompting, conditional generative modeling, planning, optimal control, and reinforcement learning, and discuss common challenges and open problems in the field.', 'categories': 'cs.ai cs.lg', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['sherry yang', 'ofir nachum', 'yilun du', 'jason wei', 'pieter abbeel', 'dale schuurmans'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04129'}, {'title': 'exploiting asymmetry for synthetic training data generation: synthie and   the case of information extraction', 'id': '2303.04132', 'abstract': 'large language models (llms) show great potential for synthetic data generation. this work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by the llm: we show that, for problems with structured outputs, it is possible to prompt an llm to perform the task in the opposite direction, to generate plausible text for the target structure. leveraging the asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. we demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. we synthetically generate a dataset of 1.8m data points, demonstrate its superior quality compared to existing datasets in a human evaluation and use it to finetune small models (220m and 770m parameters). the models we introduce, synthie, outperform existing baselines of comparable size with a substantial gap of 57 and 79 absolute points in micro and macro f1, respectively. code, data, and models are available at https://github.com/epfl-dlab/synthie.', 'categories': 'cs.cl cs.ai cs.lg', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['martin josifoski', 'marija sakota', 'maxime peyrard', 'robert west'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04132'}, {'title': 'domain randomization for robust, affordable and effective closed-loop   control of soft robots', 'id': '2303.04136', 'abstract': 'soft robots are becoming extremely popular thanks to their intrinsic safety to contacts and adaptability. however, the potentially infinite number of degrees of freedom makes their modeling a daunting task, and in many cases only an approximated description is available. this challenge makes reinforcement learning (rl) based approaches inefficient when deployed on a realistic scenario, due to the large domain gap between models and the real platform. in this work, we demonstrate, for the first time, how domain randomization (dr) can solve this problem by enhancing rl policies with: i) a higher robustness w.r.t. environmental changes; ii) a higher affordability of learned policies when the target model differs significantly from the training model; iii) a higher effectiveness of the policy, which can even autonomously learn to exploit the environment to increase the robot capabilities (environmental constraints exploitation). moreover, we introduce a novel algorithmic extension of previous adaptive domain randomization methods for the automatic inference of dynamics parameters for deformable objects. we provide results on four different tasks and two soft robot designs, opening interesting perspectives for future research on reinforcement learning for closed-loop soft robot control.', 'categories': 'cs.ro cs.lg', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['gabriele tiboni', 'andrea protopapa', 'tatiana tommasi', 'giuseppe averta'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04136'}, {'title': 'from copilot to pilot: towards ai supported software development', 'id': '2303.04142', 'abstract': \"ai-supported programming has arrived, as shown by the introduction and successes of large language models for code, such as copilot/codex (github/openai) and alphacode (deepmind). above human average performance on programming challenges is now possible. however, software engineering is much more than solving programming contests. moving beyond code completion to ai-supported software engineering will require an ai system that can, among other things, understand how to avoid code smells, to follow language idioms, and eventually (maybe!) propose rational software designs. in this study, we explore the current limitations of ai-supported code completion tools like copilot and offer a simple taxonomy for understanding the classification of ai-supported code completion tools in this space. we first perform an exploratory study on copilot's code suggestions for language idioms and code smells. copilot does not follow language idioms and avoid code smells in most of our test scenarios. we then conduct additional investigation to determine the current boundaries of ai-supported code completion tools like copilot by introducing a taxonomy of software abstraction hierarchies where 'basic programming functionality' such as code compilation and syntax checking is at the least abstract level, software architecture analysis and design are at the most abstract level. we conclude by providing a discussion on challenges for future development of ai-supported code completion tools to reach the design level of abstraction in our taxonomy.\", 'categories': 'cs.se cs.ai cs.lg', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['rohith pudari', 'neil a. ernst'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04142'}, {'title': 'can we scale transformers to predict parameters of diverse imagenet   models?', 'id': '2303.04143', 'abstract': 'pretraining a neural network on a large dataset is becoming a cornerstone in machine learning that is within the reach of only a few communities with large-resources. we aim at an ambitious goal of democratizing pretraining. towards that goal, we train and release a single neural network that can predict high quality imagenet parameters of other neural networks. by using predicted parameters for initialization we are able to boost training of diverse imagenet models available in pytorch. when transferred to other datasets, models initialized with predicted parameters also converge faster and reach competitive final performance.', 'categories': 'cs.lg cs.ai cs.cv stat.ml', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['boris knyazev', 'doha hwang', 'simon lacoste-julien'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04143'}, {'title': 'benign overfitting for two-layer relu networks', 'id': '2303.04145', 'abstract': 'modern deep learning models with great expressive power can be trained to overfit the training data but still generalize well. this phenomenon is referred to as benign overfitting. recently, a few studies have attempted to theoretically understand benign overfitting in neural networks. however, these works are either limited to neural networks with smooth activation functions or to the neural tangent kernel regime. how and when benign overfitting can occur in relu neural networks remains an open problem. in this work, we seek to answer this question by establishing algorithm-dependent risk bounds for learning two-layer relu convolutional neural networks with label-flipping noise. we show that, under mild conditions, the neural network trained by gradient descent can achieve near-zero training loss and bayes optimal test risk. our result also reveals a sharp transition between benign and harmful overfitting under different conditions on data distribution in terms of test risk. experiments on synthetic data back up our theory.', 'categories': 'cs.lg math.oc stat.ml', 'doi': '', 'created': '2023-03-07', 'updated': '', 'authors': ['yiwen kou', 'zixiang chen', 'yuanzhou chen', 'quanquan gu'], 'affiliation': [], 'url': 'https://arxiv.org/abs/2303.04145'}]\n"
     ]
    }
   ],
   "source": [
    "today_list=[]\n",
    "for i in output:\n",
    "    if i['id'][0:7]=='2303.04':\n",
    "        today_list.append(i)\n",
    "\n",
    "\n",
    "print(today_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                              title  \\\n",
      "0   2303.04001        elodin: naming concepts in embedding spaces   \n",
      "1   2303.04012         exploration via epistemic value estimation   \n",
      "2   2303.04016  decoupling skill learning from robotic control...   \n",
      "3   2303.04020  when is importance weighting correction needed...   \n",
      "4   2303.04030  pyxab -- a python library for $\\mathcal{x}$-ar...   \n",
      "5   2303.04038  root cause identification for collective anoma...   \n",
      "6   2303.04040  uncertainty quantification of spatiotemporal t...   \n",
      "7   2303.04091  visual abstraction and reasoning through language   \n",
      "8   2303.04096  mastering strategy card game (legends of code ...   \n",
      "9   2303.04104  an inception-residual-based architecture with ...   \n",
      "10  2303.04105  introspective cross-attention probing for ligh...   \n",
      "11  2303.04115  predicted embedding power regression for large...   \n",
      "12  2303.04117  validation of a hospital digital twin with mac...   \n",
      "13  2303.04118  a multiplicative value function for safe and e...   \n",
      "14  2303.04124  wigner kernels: body-ordered equivariant machi...   \n",
      "15  2303.04129  foundation models for decision making: problem...   \n",
      "16  2303.04132  exploiting asymmetry for synthetic training da...   \n",
      "17  2303.04136  domain randomization for robust, affordable an...   \n",
      "18  2303.04142  from copilot to pilot: towards ai supported so...   \n",
      "19  2303.04143  can we scale transformers to predict parameter...   \n",
      "20  2303.04145     benign overfitting for two-layer relu networks   \n",
      "\n",
      "                                             abstract  \n",
      "0   despite recent advancements, the field of text...  \n",
      "1   how to efficiently explore in reinforcement le...  \n",
      "2   recent works in robotic manipulation through r...  \n",
      "3   this paper investigates when the importance we...  \n",
      "4   we introduce a python open-source library for ...  \n",
      "5   this paper presents an approach for identifyin...  \n",
      "6   recent studies have significantly improved the...  \n",
      "7   while artificial intelligence (ai) models have...  \n",
      "8   deep reinforcement learning combined with fict...  \n",
      "9   this paper presents a deep learning system app...  \n",
      "10  we propose inca, a lightweight method for tran...  \n",
      "11  out-of-distribution (ood) inputs can compromis...  \n",
      "12  recently there has been a surge of interest in...  \n",
      "13  an emerging field of sequential decision probl...  \n",
      "14  machine-learning models based on a point-cloud...  \n",
      "15  foundation models pretrained on diverse data a...  \n",
      "16  large language models (llms) show great potent...  \n",
      "17  soft robots are becoming extremely popular tha...  \n",
      "18  ai-supported programming has arrived, as shown...  \n",
      "19  pretraining a neural network on a large datase...  \n",
      "20  modern deep learning models with great express...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cols = ['id', 'title',  'abstract']\n",
    "df = pd.DataFrame(list(today_list),columns=cols)\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
